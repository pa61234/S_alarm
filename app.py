# S_alarm/app.py

from flask import Flask, render_template, request, jsonify
from pymongo import MongoClient
from dotenv import load_dotenv
import os
from bson import ObjectId
from datetime import datetime, timezone, timedelta
import json
import feedparser
from langdetect import detect
import subprocess

from models.ner import extract_companies
from models.event_classifier import classify_event
from models.sentiment_analyzer import sentiment_analyzer

load_dotenv()
client = MongoClient(os.getenv("MONGO_URI"))
db = client["stock_alert"]

app = Flask(__name__)

# RSS í”¼ë“œ ëª©ë¡
RSS_FEEDS = {
    "ì¡°ì„ ë¹„ì¦ˆ": "https://biz.chosun.com/rss.xml",
    "ë¨¸ë‹ˆíˆ¬ë°ì´": "https://news.mt.co.kr/mtview/rss.htm",
    "í•œêµ­ê²½ì œ": "https://www.hankyung.com/feed/news",
    "ì„œìš¸ê²½ì œ": "https://www.sedaily.com/rss/Finance.xml",
    "ë§¤ì¼ê²½ì œ": "https://www.mk.co.kr/rss/30000001/",
    "ì•„ì‹œì•„ê²½ì œ": "https://www.asiae.co.kr/rss/news.xml",
    "ì´ë°ì¼ë¦¬": "https://rss.edaily.co.kr/rss/Sec_list.xml",
    "ì „ìì‹ ë¬¸": "https://rss.etnews.com/ETnews.xml",
    "ZDNet Korea": "https://zdnet.co.kr/news/news.xml",
    "ë””ì§€í„¸íƒ€ì„ìŠ¤": "https://www.dt.co.kr/rss/news.xml",
    "ì—°í•©ë‰´ìŠ¤": "https://www.yna.co.kr/news/rss",
    "KDIë‰´ìŠ¤": "https://www.kdi.re.kr/rss/kdi_news.xml",
    "íŒŒì´ë‚¸ì…œë‰´ìŠ¤": "https://www.fnnews.com/rss/fn_realnews_finance.xml",
    "í—¤ëŸ´ë“œê²½ì œ": "https://biz.heraldcorp.com/common/rss_xml.php?ct=010000000000",
    "MTN": "https://news.mtn.co.kr/newscenter/rss/news.xml",
    "í•œêµ­íˆ¬ìì¦ê¶Œ": "https://www.koreainvestment.com/rss/news.xml",
    "NHíˆ¬ìì¦ê¶Œ": "https://www.nhqv.com/rss/news.xml",
    "KBì¦ê¶Œ": "https://www.kbsec.com/rss/news.xml",
    "ì‹ í•œíˆ¬ìì¦ê¶Œ": "https://www.shinhan.com/rss/news.xml",
    "ë””ì§€í„¸ë°ì¼ë¦¬": "https://www.digitaldaily.co.kr/rss/news.xml",
    "í…Œí¬M": "https://www.techm.kr/rss/news.xml",
    "ì•„ì´ë‰´ìŠ¤24": "https://www.inews24.com/rss/news.xml",
    "ë°”ì´ì˜¤ìŠ¤í™í…Œì´í„°": "https://www.biospectator.com/rss/news.xml",
    "íŒœë‰´ìŠ¤": "https://www.pharmnews.com/rss/allArticle.xml",
    "ì‹í’ˆì €ë„": "https://www.foodnews.co.kr/rss/allArticle.xml",
    "íŒŒì´ë‚¸ì…œë‰´ìŠ¤ ì‚°ì—…": "https://www.fnnews.com/rss/fn_industry.xml",
    "ì „ìì‹ ë¬¸ ì‚°ì—…": "https://rss.etnews.com/ETnews_industry.xml"
}

# ì‚°ì—…êµ° ë§¤í•‘ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ 1íšŒë§Œ)
with open("sector_mapping.json", "r", encoding="utf-8") as f:
    SECTOR_MAP = json.load(f)

# ìœ ë‹ˆí¬í•œ ì‚°ì—…êµ° ë¦¬ìŠ¤íŠ¸
sectors = sorted(list(set(SECTOR_MAP.values())))

def get_elapsed_time(published_time):
    """ë‰´ìŠ¤ ê²Œì‹œ ì‹œê°„ìœ¼ë¡œë¶€í„°ì˜ ê²½ê³¼ì‹œê°„ì„ ê³„ì‚°"""
    now = datetime.now(timezone(timedelta(hours=9)))
    diff = now - published_time
    
    if diff.days > 0:
        return '+1day'
    elif diff.seconds >= 21600:  # 6ì‹œê°„
        return '+6hours'
    elif diff.seconds >= 10800:  # 3ì‹œê°„
        return '+3hours'
    elif diff.seconds >= 3600:  # 1ì‹œê°„
        return '+1hour'
    else:
        return 'New'

@app.route("/", methods=["GET"])
def index():
    try:
        # ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ì‹œì‘
        result = subprocess.run(['netstat', '-ano'], capture_output=True, text=True)
        if '8000' not in result.stdout:
            subprocess.Popen(['python', 'start_server.py'], 
                           stdout=subprocess.DEVNULL,
                           stderr=subprocess.DEVNULL)
            return "ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", 503

        selected_sector = request.args.get("sector", "")
        keyword = request.args.get("keyword", "")
        
        # ìƒˆë¡œìš´ ë‰´ìŠ¤ ìˆ˜ì§‘
        print("ğŸ”„ ìƒˆë¡œìš´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        for source_name, feed_url in RSS_FEEDS.items():
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries:
                    title = entry.title
                    link = entry.link
                    
                    # ì¤‘ë³µ í™•ì¸
                    if db.news.find_one({"title": title, "source": source_name}):
                        continue
                    
                    # ì‹œê°„ íŒŒì‹±
                    published = entry.get("published", "")
                    published_dt = None
                    
                    # ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ ì²˜ë¦¬
                    time_formats = [
                        "%a, %d %b %Y %H:%M:%S %z",
                        "%a, %d %b %Y %H:%M:%S %Z",
                        "%Y-%m-%dT%H:%M:%S%z",
                        "%Y-%m-%d %H:%M:%S",
                    ]
                    
                    for fmt in time_formats:
                        try:
                            published_dt = datetime.strptime(published, fmt)
                            if published_dt.tzinfo is None:
                                published_dt = published_dt.replace(tzinfo=timezone(timedelta(hours=9)))
                            elif published_dt.tzinfo.utcoffset(published_dt) != timedelta(hours=9):
                                published_dt = published_dt.astimezone(timezone(timedelta(hours=9)))
                            break
                        except ValueError:
                            continue
                    
                    if not published_dt:
                        published_dt = datetime.now(timezone(timedelta(hours=9)))
                    
                    # ì–¸ì–´ ê°ì§€
                    try:
                        lang = detect(title)
                    except:
                        lang = "unknown"
                    
                    # ê¸°ë³¸ ì €ì¥ êµ¬ì¡°
                    doc = {
                        "title": title,
                        "link": link,
                        "published": published_dt,
                        "source": source_name,
                        "lang": lang,
                    }
                    
                    # í•œêµ­ì–´ ë‰´ìŠ¤ì¸ ê²½ìš° ì¶”ê°€ ë¶„ì„
                    if lang == "ko":
                        companies = extract_companies(title)
                        if companies:
                            doc.update({
                                "companies": companies,
                                "event": classify_event(title),
                                "sentiment": sentiment_analyzer.analyze(title),
                                "analyzed": True
                            })
                    
                    db.news.insert_one(doc)
                    print(f"âœ… ìƒˆ ë‰´ìŠ¤ ì¶”ê°€: {title}")
                    
            except Exception as e:
                print(f"âš ï¸ {source_name} í”¼ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
        
        # 1. ê¸°ì—… ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ë§Œ ê°€ì ¸ì˜´
        query = {"companies": {"$exists": True, "$ne": []}}
        if selected_sector:
            query["companies"] = {"$in": [c for c, s in SECTOR_MAP.items() if s == selected_sector]}
        news = list(db.news.find(query).sort("published", -1).limit(50))  # ìµœê·¼ 50ê°œë¡œ ì œí•œ
        
        # 2. í‚¤ì›Œë“œ í•„í„°ë§
        if keyword:
            news = [item for item in news if keyword in item["title"]]
        
        # 3. ê¸°ì—… ì •ë³´ê°€ ì—†ëŠ” ë‰´ìŠ¤ ì¤‘ì—ì„œ í•œêµ­ì–´ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì™€ì„œ ê¸°ì—… ì •ë³´ ì¶”ì¶œ
        if len(news) < 10:  # ê¸°ì—… ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ 10ê°œ ë¯¸ë§Œì¸ ê²½ìš°
            # ë” ë§ì€ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ limit ì¦ê°€
            additional_news = list(db.news.find({
                "lang": "ko",
                "companies": {"$exists": False}  # companies í•„ë“œê°€ ì—†ëŠ” ê²½ìš°
            }).sort("published", -1).limit(50))
            
            for item in additional_news:
                # ê¸°ì—…ëª… ì¶”ì¶œ
                companies = extract_companies(item['title'])
                if companies:  # ê¸°ì—…ëª…ì´ ì¶”ì¶œëœ ê²½ìš°ì—ë§Œ ì¶”ê°€
                    item['companies'] = companies
                    item['event'] = classify_event(item['title'])
                    item['sentiment'] = sentiment_analyzer.analyze(item['title'])
                    news.append(item)
                    
                    # DBì— ì—…ë°ì´íŠ¸
                    db.news.update_one(
                        {"_id": item['_id']},
                        {"$set": {
                            "companies": companies,
                            "event": item['event'],
                            "sentiment": item['sentiment'],
                            "analyzed": True
                        }}
                    )
                
                # 10ê°œ ì´ìƒì´ë©´ ì¤‘ë‹¨
                if len(news) >= 10:
                    break
        
        print(f"[DEBUG] ê°€ì ¸ì˜¨ ë‰´ìŠ¤ ìˆ˜: {len(news)}")
        print(f"[DEBUG] ì „ì²´ ë‰´ìŠ¤ ìˆ˜: {db.news.count_documents({})}")
        print(f"[DEBUG] í•œêµ­ì–´ ë‰´ìŠ¤ ìˆ˜: {db.news.count_documents({'lang': 'ko'})}")
        print(f"[DEBUG] ê¸°ì—… ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜: {db.news.count_documents({'companies': {'$exists': True, '$ne': []}})}")
        
        for n in news:
            print(f"[DEBUG] ë‰´ìŠ¤: {n['title']} | {n.get('lang', 'unknown')} | {n.get('companies', [])}")
            
        for item in news:
            item['_id'] = str(item['_id'])  # ObjectIdë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            # publishedë¥¼ KSTë¡œ ë³€í™˜
            if item['published'].tzinfo is None:
                item['published'] = item['published'].replace(tzinfo=timezone(timedelta(hours=9)))
            elif item['published'].tzinfo.utcoffset(item['published']) != timedelta(hours=9):
                item['published'] = item['published'].astimezone(timezone(timedelta(hours=9)))
            # link í•„ë“œë¥¼ urlë¡œ ë§¤í•‘
            item['url'] = item.get('link', '#')
            # ê²½ê³¼ì‹œê°„ ê³„ì‚°
            item['elapsed_time'] = get_elapsed_time(item['published'])
            # ê°ì„±ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            item['sentiment'] = item.get('sentiment', 'unknown')
        return render_template("index.html", news=news, sectors=sectors, selected_sector=selected_sector, keyword=keyword)
    except Exception as e:
        print(f"[ERROR] {str(e)}")
        return str(e), 500

@app.route("/analyze", methods=["POST"])
def analyze():
    data = request.json
    news_id = data.get("news_id")
    title = data.get("title")

    if not news_id or not title:
        return jsonify({"status": "error", "message": "invalid input"}), 400

    try:
        companies = extract_companies(title)
        event = classify_event(title)

        # ì‚°ì—…êµ° ì¶”ì¶œ
        sector = ""
        for company in companies:
            if company in SECTOR_MAP:
                sector = SECTOR_MAP[company]
                break

        db.news.update_one(
            {"_id": ObjectId(news_id)},
            {"$set": {
                "companies": companies,
                "event": event,
                "sector": sector,
                "analyzed": True
            }}
        )

        return jsonify({"status": "success"})
    except Exception as e:
        print(f"[ERROR] ë¶„ì„ ì‹¤íŒ¨: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route("/feedback", methods=["POST"])
def feedback():
    data = request.json
    news_id = data.get("news_id")
    feedback = data.get("feedback")

    if not news_id or not feedback:
        return jsonify({"status": "error", "message": "invalid input"}), 400

    db.feedback.insert_one({
        "news_id": ObjectId(news_id),
        "feedback": feedback,
        "timestamp": datetime.utcnow()
    })

    return jsonify({"status": "success"})

if __name__ == "__main__":
    import threading
    import webbrowser
    import time
    
    def open_browser():
        time.sleep(1.5)  # ì„œë²„ê°€ ì™„ì „íˆ ì‹œì‘ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
        webbrowser.open('http://127.0.0.1:8000')
    
    # ë¸Œë¼ìš°ì €ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì—´ê¸°
    threading.Thread(target=open_browser).start()
    
    # ì„œë²„ ì‹œì‘
    app.run(debug=True, port=8000, use_reloader=False)
